{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In below cell, we import required python packages which will help us to write python script to perfrom model training.\n",
        "Here we are majorly importing 2 main packages - \n",
        "\n",
        "\n",
        "1.   `torch`: main python package containing pytorch Deep Lerning framework. It will help us to define layers, optimizers, loss, activation, etc.\n",
        "2.   `__future__`: This is more of a utility package help us to use features avaliable in new python.\n",
        "3. `torchvision`: This python package focuses on computer vision related utility for torch. This package contains popular dataset for computer vision, famous model architecture and some utility to form image transformation.\n",
        "\n"
      ],
      "metadata": {
        "id": "ijtGoTAp52Pz"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m2JWFliFfKT"
      },
      "source": [
        "# we import print_function from __future__. This will help us to use newer print utility.\n",
        "from __future__ import print_function \n",
        "\n",
        "# we are importing torch package. This will give us access to different modules.\n",
        "import torch \n",
        "\n",
        "# nn(neural network) is a base module from torch pacakge. It contains all utility module to build a neural network.\n",
        "import torch.nn as nn \n",
        "\n",
        "# `torch.nn.functional` module contains functions that are commonly used in neural network. \n",
        "#This module is often used in conjunction with the `torch.nn` module\n",
        "import torch.nn.functional as F \n",
        "\n",
        "# here we are importing optimization module from `torch` module and naming it as optim. \n",
        "# optimization modules contains different type of optimization which helps in updating the paramerters during model training.\n",
        "import torch.optim as optim\n",
        "\n",
        "# finally we are importing datasets and transforms sub-module from torchvision module.\n",
        "# datasets module will have collection of popular datasets like MNIST, COCO, etc.\n",
        "# transforms module will have functions to perform pre-procssing on datasets like normalization, augmentation, etc.\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In below cell, we create a custom class called `Net`. We create this class by inhereting other class called `Module` from `nn` sub-module. \n",
        "Here `Module` class is the base class for all neural network modules in `nn` package. `Module` class has useful function like `parameters()` and `train()` which are used to access and manipulate the parameters of the network.\n",
        "In our custom class `Net` we will define following things-\n",
        "\n",
        "\n",
        "1.   Our Neural Network architecutre by specifing each layer.\n",
        "2.   forward function which will help us to perform forward propogation to calculate our predicted value.\n",
        "3. we also define activation funcation in our NN architecture. \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "i8enyDU1C78z"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_Cx9q2QFgM7"
      },
      "source": [
        "class Net(nn.Module): # define class called `Net` by inheriting `nn.module` class \n",
        "    def __init__(self): # define class constructure, This function is called when we create an instance of this class\n",
        "        super(Net, self).__init__() # here we are initlizing parent class of Net which is nn.module, this will set internal state of parent class.\n",
        "\n",
        "        #Below we are initilzing all layers required for our Neural Network from nn module. \n",
        "\n",
        "        # input_size = 28x28x1, output_size = 28x28x32\n",
        "        # input_channel = 1, output_channel/no. of kernels = 32, kernel_size = 3, padding=1, receptive_field = 2x2 (it is not 3 because we remove padding part from receptive field)\n",
        "        # here due to padding input and output size (not considering the channel) is same i.e. 28x28\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1) \n",
        "\n",
        "        # input_size = 28x28x32, output_size = 28x28x64\n",
        "        # input_channel = 32, output_channel/no. of kernels = 64, kernel_size = 3, padding=1, receptive_field = 3x3\n",
        "        # due to padding input and output are still same i.e. 28\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "\n",
        "        # input_size = 28x28x64, output_size = 14x14x64\n",
        "        # input_channel = 64, output_channel/no. of kernels = remains same, filter_size = 2, stride=2, padding=0, receptive_field = 9x9\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # input_size = 14x14x64, output_size = 14x14x128\n",
        "        # input_channel = 64, output_channel/no. of kernels = 128, filter_size = 3, receptive_field = 10x10\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "\n",
        "        # input_size = 14x14x128, output_size = 14x14x256\n",
        "        # input_channel = 128, output_channel/no. of kernels = 256, filter_size = 3, receptive_field = 11x11\n",
        "        self.conv4 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "\n",
        "        # input_size = 14x14x256, output_size = 7x7x256\n",
        "        # input_channel = 256, output_channel/no. of kernels = remains same, filter_size = 2, stride=2, padding=0, receptive_field = 22x22\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        # input_size = 7x7x256, output_size = 5x5x512\n",
        "        # input_channel = 256, output_channel/no. of kernels = 512, filter_size = 3, padding=0, receptive_field = 24x24 (now it not 23 because padding is not added)\n",
        "        self.conv5 = nn.Conv2d(256, 512, 3)\n",
        "\n",
        "        # input_size = 5x5x512, output_size = 3x3x1024\n",
        "        # input_channel = 512, output_channel/no. of kernels = 1024, filter_size = 3, receptive_field = 26x26\n",
        "        self.conv6 = nn.Conv2d(512, 1024, 3)\n",
        "\n",
        "        # input_size = 3x3x1024, output_size = 1x1x10\n",
        "        # input_channel = 1024, output_channel/no. of kernels = 10, filter_size = 3, receptive_field = 28x28 (equal to the size of image!!)\n",
        "        # here output_channel also represent the no. of class we need to predict.\n",
        "        self.conv7 = nn.Conv2d(1024, 10, 3)\n",
        "    \n",
        "    # Below function `forward()` is responsible for connecting the layers we defined and generate output while propogating through this layers\n",
        "    def forward(self, x): #defining forward function to take x which is the input vector.\n",
        "\n",
        "        # here x -> conv1 -> F.relu -> conv2 -> F.relu -> pool1 -> update value of x\n",
        "        # here F.relu is activation function, it helps to impart non-linarity and state of the neuron\n",
        "        x = self.pool1(F.relu(self.conv2(F.relu(self.conv1(x)))))\n",
        "\n",
        "        # here updated x -> conv3 -> F.relu -> conv4 -> F.relu -> pool2 -> again update value of x\n",
        "        x = self.pool2(F.relu(self.conv4(F.relu(self.conv3(x)))))\n",
        "\n",
        "        # update x -> conv5 -> F.relu -> convn6 -> F.relu -> update x again\n",
        "        x = F.relu(self.conv6(F.relu(self.conv5(x))))\n",
        "\n",
        "        # updated x -> conv7 -> update value of x\n",
        "        # here we have total of 7 convolution layers and two pooling layer\n",
        "        x = F.relu(self.conv7(x))\n",
        "\n",
        "        # we finally flatten the output of 1x1x10 into a single vector of 10 neurons.\n",
        "        x = x.view(-1, 10)\n",
        "\n",
        "        # we return the value after perfoming softmax activation.\n",
        "        # we use softmax activation in final layer when we are working with mutli class problem.\n",
        "        return F.log_softmax(x)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In below cell we are installing torchsummary python package and setting up GPU device to efficiently train model.\n",
        "\n",
        "\n",
        "*   `torchsummary` is a python package which contain's utility function to get infromation about model architecture, model summary, input, output, etc.\n",
        "\n",
        "*   In `torch` we can use cuda devices like GPU, TPU to trian the model faster. Below cell provide code to initilize cuda GPU's \n",
        "\n"
      ],
      "metadata": {
        "id": "h_-WGuVAlhrU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdydjYTZFyi3"
      },
      "source": [
        "!pip install torchsummary # install torchsummary python pacakge \n",
        "from torchsummary import summary # import summary module from torchsummary, it will contain required utility function\n",
        "\n",
        "# we check if cuda device is avaialble or not in the given system.\n",
        "# use_cuda=1 if device is there & use_cuda=0 if there is no device avaliable\n",
        "use_cuda = torch.cuda.is_available() \n",
        "\n",
        "# we setup device based on availablity\n",
        "# if cuda is there then we will use cuda GPU else we will use CPU \n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "# Here we are creating an instance of our custom class Net() called model.\n",
        "# we are calling .to(device) method on our Net() instance to set all tensors and parameters to compute on selected device\n",
        "model = Net().to(device)\n",
        "\n",
        "# here we are using our utility function to print our our model summary, no. of parameter, inputs, outputs, etc.\n",
        "summary(model, input_size=(1, 28, 28))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Till now we have defined our model and device over which we want to train our model. \n",
        "Now we will look into how to define data, batch size and how to load data for training. \n",
        "\n",
        "*   In below code, we create an instance of `torch.utils.data.DataLoader` module. Here dataloader helps us to maintain the flow of data from storage to memory on fly and helps to manage pre-processing of data. Below are the arguments for DataLoader.\n",
        "  1. `dataset` - dataset object which need to be created, here we are creating MNIST dataset object.\n",
        "  2. `batch_size` - batch size or no. of images over which model parameters will be updated.\n",
        "  3. `shuffle` - flag to specifiy wheather dataset is needed to shuffle. It helps to break any kind of bias related to the order of image used during training.\n",
        "  \n",
        "*   we use `batch_size` to set number of input images to use during optimization. This are the no. of images over which model will update it's parameter. If model update it's parameter after each image it is called as schostic optimization and if we update parameters after passing through all image we call it batch optimiation. It is always better to batch_size of power of 2 i.e. 16, 64, 128, etc.\n",
        "\n",
        "* Here we use existing MNIST dataset from datasets module. Below are arguments which we used in datasets.MNIST().\n",
        "  1. Path wher MNIST dataset needs to be downloaded.\n",
        "  2. `train` flag which specific wheather dataset instance will be used for training or evaluation.\n",
        "  3. `transform.to_tensor()` which will transform image data into pytorch tensors.\n",
        "  4. `transforms.Normalize()` to normalize image i.e. scaling rgb value  from 0-255 to other scale depending on provided mean and standerd deviation.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-84LS3MqG6Ny"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqTWLaM5GHgH"
      },
      "source": [
        "\n",
        "\n",
        "torch.manual_seed(1) # since lot's of values will be initilized randomly, this line help us to generate similar random values in each run.\n",
        "\n",
        "batch_size = 128 # setting batch size to 128 means model will update parameters after every 128 image pass.\n",
        "\n",
        "# if we are using cuda based GPU and generate kwargs dictionary.\n",
        "# here num_workers are no. of worker processes to use for data loading, setting more then 0 will help to parallelize data loading.\n",
        "# pin_memory is a special type of memory that is faster to access compare to regular memory, it can be used to speed up data transfer between CPU and GPU.\n",
        "# setting pin_memory to True means allow dataloader to use this special memory.\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "# creating dataloader instance which will be used for training the model.\n",
        "# here we are using available dataset MNIST\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                    transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "#creating dataloader instance which will be used for evaluating the model\n",
        "# we set train to False to signify that it is for evaluation.\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In below cell we are defining our main `train()` and `test()` function which will be responsible for training and evaluating our model.\n",
        "\n",
        "Our `train()` and `test()` function takes following input arguments:\n",
        "*  model - model which is an instance of our `Net()` module which contains model architecture.\n",
        "*  device - device over which we will be training our neural network.\n",
        "*  train_loader - DataLoader instance created for handling training dataset.\n",
        "*  optimizer - which optimizer to use for performing gradient decent like Adam, SGD, GD with momentum, etc.\n",
        "*  epoch - number of iteration over complete dataset. Each iteration consist of forward and backward pass while training the model. 20 epoch means model will go throught complete dataset 20 times. \n",
        "\n"
      ],
      "metadata": {
        "id": "H9kh_9EbQcwW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fDefDhaFlwH"
      },
      "source": [
        "from tqdm import tqdm # a python library utility libray which provide progress-bar for iterative function\n",
        "\n",
        "# define train() function which will be responsible for forward and backward pass for training\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train() # call train() method on model instance to set internel train flag as true. helps to initlize certain layers.\n",
        "    pbar = tqdm(train_loader) # object to show progress-bar while iterating through train_loader\n",
        "    for batch_idx, (data, target) in enumerate(pbar): # iterate through train_loader and unpack data and ground truth\n",
        "        data, target = data.to(device), target.to(device) # set computer device for data and target tensor\n",
        "        optimizer.zero_grad() # initilize gradients of optimizer to zero\n",
        "        output = model(data) # perfrom forward pass or prediction on given data and model\n",
        "        loss = F.nll_loss(output, target) # calculate loss using predicted value and ground truth, we are using negative log likelihood loss\n",
        "        loss.backward() # now we perform backward pass to calculate gradients \n",
        "        optimizer.step() # this will update the value of parameters/weights\n",
        "        pbar.set_description(desc= f'loss={loss.item()} batch_id={batch_idx}') # display loss of each iteration.\n",
        "\n",
        "# define test() function which will be responsible to evaluate model performance on non-training data.\n",
        "def test(model, device, test_loader):\n",
        "    model.eval() # will set model into evaluate state i.e. setting training flag to False\n",
        "    test_loss = 0 \n",
        "    correct = 0\n",
        "    with torch.no_grad(): # switch off gradient computation\n",
        "        for data, target in test_loader: # iterate through test_loader datasets\n",
        "            data, target = data.to(device), target.to(device) # set device for tensor\n",
        "            output = model(data) # perfrom prediction on test data\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # calcuate and sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item() # count number of correct predection using target/ground-truth and predicted value.\n",
        "\n",
        "    test_loss /= len(test_loader.dataset) # calculate avearge loss across no. of datasets\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))  # print loss and accuracy on the terminal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Below cell, we first create our architecture instance `model` then we define our optimizer which is SGD (stochastic Gradient Descent) with learning-rate = 0.01 and momentum=0.9.\n",
        "  * Here learning-rate control by how much the parameters value should be updated. larger learning-rate means parameters will be updated by larger values.\n",
        "  * momentum help us to avoid converging in local minimua.\n",
        "\n",
        "In this cell we are also performing epoch iteration and in each iteration we are calling train() and test() fucntion."
      ],
      "metadata": {
        "id": "tvJaj2m9hTRQ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMWbLWO6FuHb"
      },
      "source": [
        "\n",
        "model = Net().to(device) # initilize instance of Net() and then set which device to use\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) # set optimizer for training\n",
        "\n",
        "# iterate through epochs, here each epoch will again iterate through all training data.\n",
        "for epoch in range(1, 2):\n",
        "    train(model, device, train_loader, optimizer, epoch) # call train() function for training\n",
        "    test(model, device, test_loader) # call test() function for evaluating model on test dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "So5uk4EkHW6R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4560a353-7182-4606-a0d1-ff0b0b2eea44"
      },
      "source": [],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Dec  5 18:42:39 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   65C    P0    31W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RCNOolh4mOI5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}